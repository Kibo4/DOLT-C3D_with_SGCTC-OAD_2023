{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def m_eye(n, k=0):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n: size of the matrix\n",
    "    :param k: the shift value\n",
    "    :return: a identity matrix where the ones aren shifted (to right) by k\n",
    "     n=10 , k = 1,\n",
    "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "    \"\"\"\n",
    "    return tf.cast(tf.concat((tf.concat(\n",
    "                                (tf.zeros([n-k, k]),  tf.eye(n-k)), axis=1),\n",
    "                         tf.zeros([k, n])), axis=0),tf.float32)\n",
    "# def ctc_ent_loss(pred, token, pred_len,token_len, blank=0):\n",
    "pred = tf.convert_to_tensor([[\n",
    "\t# BLANK     A\tE\t\tL\t\tP\n",
    "    [0.0,      0.8,\t0, \t\t0, \t\t0.2],\n",
    "    [0.0,      0.7, 0, \t\t0.3, \t0],\n",
    "    [0.9,      0, \t0, \t\t0, \t\t0.1],\n",
    "    [0.1,      0, \t0, \t\t0, \t\t0.9],\n",
    "    [0.9,      0, \t0, \t\t0.1, \t0],\n",
    "    [0.0,      0.1, 0.1, \t0, \t\t0.8],\n",
    "    [0.1,      0, \t0, \t\t.8, \t0.1],\n",
    "    [0.0,      0,\t.8, \t.1, \t0.1]\n",
    "],[\n",
    "\t# BLANK     A\tE\t\tL\t\tP\n",
    "    [0.0,      0.8,\t0, \t\t0, \t\t0.2],\n",
    "    [0.0,      0.7, 0, \t\t0.3, \t0],\n",
    "    [0.9,      0, \t0, \t\t0, \t\t0.1],\n",
    "    [0.1,      0, \t0, \t\t0, \t\t0.9],\n",
    "    [0.9,      0, \t0, \t\t0.1, \t0],\n",
    "    [0.0,      0.1, 0.1, \t0, \t\t0.8],\n",
    "    [0.1,      0, \t0, \t\t.8, \t0.1],\n",
    "    [0.0,      0,\t.8, \t.1, \t0.1]\n",
    "]],tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([2, 8, 5])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred = pred[:,:]\n",
    "pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "token = tf.convert_to_tensor([[1,4,4,3,2],[1,4,4,3,2]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "if True:\n",
    "    '''\n",
    "    :param pred: ( batch,Time, voca_size+1)\n",
    "    :param pred_len: (batch,1)\n",
    "    :param token: (batch, U), U : length maximal of sequence in batch, padded with ?, length known with token_len\n",
    "    :param token_len: (batch,1)\n",
    "    # blank seem the index of the blank, but use T.zeros for token_with_blank\n",
    "\n",
    "    :out alpha: (Time, batch, 2U+1) ∑p(π|x)\n",
    "    :out beta: (Time, batch, 2U+1)  ∑p(π|x)logp(π|x)\n",
    "    :out H: -beta/alpha+log(alpha)\n",
    "    '''\n",
    "    # print(pred)\n",
    "    pred = tf.transpose(pred,[1,0,2]) # ( Time,batch, voca_size+1)\n",
    "    Time, batch = tf.shape(pred)[0], tf.shape(pred)[1]\n",
    "    U = tf.shape(token)[1]\n",
    "    eps = 1E-8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "    # token_with_blank\n",
    "    # \"view\" is a kind of reshape with sharing memory, but here the original concat is not kept, so same as reshape i guess\n",
    "    token_with_blank = tf.concat((tf.zeros([batch, U, 1],dtype=tf.int32), tf.cast(token[:, :, tf.newaxis],tf.int32)),axis=2)  # (batch, U,2)\n",
    "    token_with_blank = tf.reshape(token_with_blank,[batch,-1])# (batch, 2*U)\n",
    "    # add a blank at the end of elems\n",
    "    token_with_blank = tf.concat((token_with_blank, tf.zeros([batch, 1],dtype=tf.int32)), axis=1)  # (batch, 2U+1)\n",
    "    # token_with_blank: [blank, index_e1,blank, index_e2, .... eU, blank]\n",
    "\n",
    "    length = tf.shape(token_with_blank)[1]  # 2U+1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 1 0 4 0 4 0 3 0 2 0]\n",
      " [0 1 0 4 0 4 0 3 0 2 0]], shape=(2, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(token_with_blank)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "if True:\n",
    "    # take the scores of elements of the token, at each step for each batch\n",
    "    \"\"\"\n",
    "    In[24]:    a\n",
    "    Out[24]:\n",
    "    <shape=(1 (Time), 2 (batch), 5),\n",
    "    array([[[0.9688189 , 0.01720107, 0.5965241 , 0.76499116, 0.6780566 ],\n",
    "            [0.08494377, 0.34031677, 0.92728245, 0.83308065, 0.17265546]]],\n",
    "    In[25]:\n",
    "    tf.gather(a,tf.convert_to_tensor([\n",
    "                                        [0,2,4], #batch1\n",
    "                                        [0,1,4]] #batch2\n",
    "                                    )[tf.newaxis,:],axis=2,batch_dims=2)\n",
    "    Out[25]:\n",
    "    <shape=(1, 2, 3),\n",
    "    array([[[0.9688189 , 0.5965241 , 0.6780566 ],\n",
    "            [0.08494377, 0.34031677, 0.17265546]]]\n",
    "    \"\"\"\n",
    "    pred = tf.gather(pred,tf.repeat(token_with_blank[tf.newaxis,:,:],Time,axis=0),axis=2,batch_dims=2)  # (T, batch, 2U+1)]\n",
    "    # print(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.  0.8 0.  0.2 0.  0.2 0.  0.  0.  0.  0. ]\n",
      "  [0.  0.8 0.  0.2 0.  0.2 0.  0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.7 0.  0.  0.  0.  0.  0.3 0.  0.  0. ]\n",
      "  [0.  0.7 0.  0.  0.  0.  0.  0.3 0.  0.  0. ]]\n",
      "\n",
      " [[0.9 0.  0.9 0.1 0.9 0.1 0.9 0.  0.9 0.  0.9]\n",
      "  [0.9 0.  0.9 0.1 0.9 0.1 0.9 0.  0.9 0.  0.9]]\n",
      "\n",
      " [[0.1 0.  0.1 0.9 0.1 0.9 0.1 0.  0.1 0.  0.1]\n",
      "  [0.1 0.  0.1 0.9 0.1 0.9 0.1 0.  0.1 0.  0.1]]\n",
      "\n",
      " [[0.9 0.  0.9 0.  0.9 0.  0.9 0.1 0.9 0.  0.9]\n",
      "  [0.9 0.  0.9 0.  0.9 0.  0.9 0.1 0.9 0.  0.9]]\n",
      "\n",
      " [[0.  0.1 0.  0.8 0.  0.8 0.  0.  0.  0.1 0. ]\n",
      "  [0.  0.1 0.  0.8 0.  0.8 0.  0.  0.  0.1 0. ]]\n",
      "\n",
      " [[0.1 0.  0.1 0.1 0.1 0.1 0.1 0.8 0.1 0.  0.1]\n",
      "  [0.1 0.  0.1 0.1 0.1 0.1 0.1 0.8 0.1 0.  0.1]]\n",
      "\n",
      " [[0.  0.  0.  0.1 0.  0.1 0.  0.1 0.  0.8 0. ]\n",
      "  [0.  0.  0.  0.1 0.  0.1 0.  0.1 0.  0.8 0. ]]], shape=(8, 2, 11), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "    print(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 1 0 4 0 4 0 3 0 2 0]\n",
      " [0 1 0 4 0 4 0 3 0 2 0]], shape=(2, 11), dtype=int32)\n",
      "    tf.Tensor(\n",
      "[[0 1 0 4 0 4 0 3 0]\n",
      " [0 1 0 4 0 4 0 3 0]], shape=(2, 9), dtype=int32)\n",
      "    tf.Tensor(\n",
      "[[0 4 0 4 0 3 0 2 0]\n",
      " [0 4 0 4 0 3 0 2 0]], shape=(2, 9), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0 0 0 1 0 0 0 1 0 1 0]\n",
      " [0 0 0 1 0 0 0 1 0 1 0]], shape=(2, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "    # ## recurrence relation\n",
    "    # take elements with (not) consecutive same token -> 'aa' -> need to pass by the blank\n",
    "    # True/1 = different, False/0 = same\n",
    "    print(token_with_blank)\n",
    "    print(\"   \",token_with_blank[:, :-2])\n",
    "    print(\"   \",token_with_blank[:, 2:])\n",
    "    consecutiveDifferent = tf.cast(tf.not_equal(token_with_blank[:, :-2], token_with_blank[:, 2:]),tf.float32)\n",
    "\n",
    "    # pad with two blanks on the left\n",
    "    consecutiveDifferent = tf.concat(( tf.zeros((batch, 2),dtype=tf.float32), consecutiveDifferent), axis=1)\n",
    "    print(tf.cast(consecutiveDifferent,tf.int32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 1 0 4 0 4 0 3 0 2 0]\n",
      " [0 1 0 4 0 4 0 3 0 2 0]], shape=(2, 11), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]], shape=(2, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "    print(token_with_blank)\n",
    "    # elements not blank in the GT (one on two)\n",
    "    notBlanksToken = tf.cast(tf.not_equal(token_with_blank, 0),tf.float32)\n",
    "    print(notBlanksToken)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]], shape=(2, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "    # kind of mask : True = Not consecutive identic elements, False = consecutive identic elements: can skip the blank,\n",
    "    sec_diag = consecutiveDifferent * notBlanksToken  # (batch, 2U+1)\n",
    "    print(sec_diag)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]], shape=(2, 11, 11), dtype=float32)\n",
      "[2 11 11] [2 11 11] [2 11]\r\n",
      "tf.Tensor(\n",
      "[[[1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.8 1.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.8 1.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.8 1.  1.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.8 1.  1.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 1. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. ]]\n",
      "\n",
      " [[1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.8 1.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.8 1.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.8 1.  1.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.8 1.  1.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.8 1. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. ]]], shape=(2, 11, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "    # m_eye : identity matrix for k=0, ones of are shifted by k\n",
    "\n",
    "    # recurrence_relation : (batch, 2U+1, 2U+1)\n",
    "    recurrence_relation = \\\n",
    "        tf.repeat((tf.eye(length) + m_eye(length, k=1))[tf.newaxis], repeats = batch,axis=0) + \\\n",
    "        tf.repeat(  m_eye(length, k=2)[tf.newaxis],                  repeats = batch,axis=0) * sec_diag[:, tf.newaxis, :]\n",
    "    # tf.print(\"recurrence_relation\\n\",recurrence_relation,summarize=-1)\n",
    "    print(tf.squeeze(recurrence_relation))\n",
    "    ### end recurrence relation\n",
    "    #%%\n",
    "    # print((tf.cast(tf.logical_not(tf.eye(length,dtype=tf.bool)),tf.float32))+((tf.transpose(tf.cast(tf.logical_not(tf.cast(notBlanksToken,tf.bool)),tf.float32)*(0.8)))))\n",
    "    eyed = tf.repeat(tf.eye(length)[tf.newaxis], repeats = batch,axis=0)\n",
    "    tf.print(tf.shape(recurrence_relation),tf.shape(eyed),tf.shape(notBlanksToken))\n",
    "    # print(recurrence_relation,tf.eye(length, dtype=tf.float32),notBlanksToken)\n",
    "    weightInverse = 0.2\n",
    "    print(recurrence_relation*(1-(eyed)*notBlanksToken[:,:,tf.newaxis]*weightInverse))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 11, 11), dtype=float32, numpy=\narray([[[-0.e+00, -0.e+00, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08,\n         -1.e+08, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -0.e+00, -0.e+00, -0.e+00, -1.e+08, -1.e+08, -1.e+08,\n         -1.e+08, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -1.e+08, -0.e+00, -0.e+00, -1.e+08, -1.e+08, -1.e+08,\n         -1.e+08, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -0.e+00, -0.e+00, -1.e+08, -1.e+08,\n         -1.e+08, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -0.e+00, -0.e+00, -1.e+08,\n         -1.e+08, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -0.e+00, -0.e+00,\n         -0.e+00, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -0.e+00,\n         -0.e+00, -1.e+08, -1.e+08, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08,\n         -0.e+00, -0.e+00, -0.e+00, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08,\n         -1.e+08, -0.e+00, -0.e+00, -1.e+08],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08,\n         -1.e+08, -1.e+08, -0.e+00, -0.e+00],\n        [-1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08, -1.e+08,\n         -1.e+08, -1.e+08, -1.e+08, -0.e+00]]], dtype=float32)>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    eps_nan = -1e8\n",
    "\n",
    "    eps_nan * (tf.ones_like(recurrence_relation) - recurrence_relation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0. ]], shape=(1, 11), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "    # alpha\n",
    "    # initialisation, two first char of seq (blank and first elem) initalised with the direct score of prediction\n",
    "    \"\"\"\n",
    "    T.cat((pred[0,:,:2],T.zeros(batch,5-1)),dim=1)\n",
    "        tensor([\n",
    "        [0.9503, 0.3439, 0.0000, 0.0000, 0.0000, 0.0000], # 1er elem of batch\n",
    "        [0.7988, 0.6641, 0.0000, 0.0000, 0.0000, 0.0000]]) # 2nd elem of batch\n",
    "\n",
    "    --> For t = 0, take blank and first elem of token\n",
    "    \"\"\"\n",
    "    alpha_t = tf.concat((pred[0, :, :2], tf.zeros([batch, 2 * U + 1 - 2],dtype=tf.double)), axis=1)  # (batch, 2U+1)\n",
    "    # same but score*log(score) : this is the regularisation\n",
    "    beta_t = tf.concat((pred[0, :, :2] * tf.math.log(pred[0, :, :2]), tf.zeros([batch, 2 * U - 1],dtype=tf.double)),\n",
    "                   axis=1)  # (batch, 2U+1)\n",
    "    print(alpha_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.  0.8 0.  0.2 0.  0.2 0.  0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.7 0.  0.  0.  0.  0.  0.3 0.  0.  0. ]]\n",
      "\n",
      " [[0.9 0.  0.9 0.1 0.9 0.1 0.9 0.  0.9 0.  0.9]]\n",
      "\n",
      " [[0.1 0.  0.1 0.9 0.1 0.9 0.1 0.  0.1 0.  0.1]]\n",
      "\n",
      " [[0.9 0.  0.9 0.  0.9 0.  0.9 0.1 0.9 0.  0.9]]\n",
      "\n",
      " [[0.  0.1 0.  0.8 0.  0.8 0.  0.  0.  0.1 0. ]]\n",
      "\n",
      " [[0.1 0.  0.1 0.1 0.1 0.1 0.1 0.8 0.1 0.  0.1]]\n",
      "\n",
      " [[0.  0.  0.  0.1 0.  0.1 0.  0.1 0.  0.8 0. ]]], shape=(8, 1, 11), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "    print(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    alphas = tf.zeros([Time,batch,length])\n",
    "    betas = tf.zeros([Time,batch,length])\n",
    "    # will store all the alphas for all times, first do it for t=0\n",
    "    \"\"\"\n",
    "    alpha_t =  tf.convert_to_tensor([[1,2,0,0],[5,6,0,0]],dtype=tf.float32)\n",
    "    tf.tensor_scatter_nd_update(tf.zeros(shape=(3, 2, 4)), [[0]],alpha_t[tf.newaxis])\n",
    "    Out:\n",
    "    <tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
    "    array([[[1., 2., 0., 0.],\n",
    "            [5., 6., 0., 0.]],\n",
    "           [[0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0.]],\n",
    "           [[0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0.]]], dtype=float32)>\n",
    "    \"\"\"\n",
    "    alphas = tf.tensor_scatter_nd_update(alphas, [[0]], alpha_t[tf.newaxis])\n",
    "    betas = tf.tensor_scatter_nd_update(betas, [[0]], beta_t[tf.newaxis])\n",
    "    # tf.print(\"pred\",pred,summarize=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # dynamic programming\n",
    "    # (T, batch, 2U+1)\n",
    "    def do(t):\n",
    "        global alpha_t\n",
    "        global beta_t\n",
    "        alpha_t = tf.matmul(alpha_t[:, tf.newaxis], recurrence_relation)[:, 0] * pred[t, :, :]\n",
    "        beta_t = tf.matmul(beta_t[:, tf.newaxis], recurrence_relation)[:, 0] * pred[t, :, :] + tf.math.log(\n",
    "            pred[t] + eps) * alpha_t\n",
    "        global alphas\n",
    "        global betas\n",
    "        alphas = tf.tensor_scatter_nd_update(alphas, [[t]], alpha_t[tf.newaxis])\n",
    "        betas = tf.tensor_scatter_nd_update(betas, [[t]], beta_t[tf.newaxis])\n",
    "        return tf.add(t, 1)\n",
    "\n",
    "    tf.map_fn(do,tf.range(1, Time,dtype=tf.int32))\n",
    "    #\n",
    "    # for t in tf.range(1, Time,dtype=tf.int32):\n",
    "    #     alpha_t = tf.matmul(alpha_t[:, tf.newaxis], recurrence_relation)[:, 0] * pred[t,:,:]\n",
    "    #     beta_t = tf.matmul(beta_t[:, tf.newaxis], recurrence_relation)[:, 0] * pred[t,:,:] + tf.math.log(pred[t]+eps) * alpha_t\n",
    "    #\n",
    "    #     alphas = tf.tensor_scatter_nd_update(alphas, [[t]], alpha_t[tf.newaxis])\n",
    "    #     betas = tf.tensor_scatter_nd_update(betas, [[t]], beta_t[tf.newaxis])\n",
    "\n",
    "    # tf.print(\"alphas\", Time, alphas)\n",
    "    # tf.print(\"beta_t\", Time-1, beta_t)\n",
    "\n",
    "    def collect_label(probability):\n",
    "        probability = tf.transpose(probability,[1,0,2])\n",
    "        #pred len : [batch,1]\n",
    "        probability = tf.gather(probability, pred_len-1, axis=1, batch_dims=1)[:,0,:]\n",
    "        #token_len : [batch,1]\n",
    "        labels_2=tf.gather(probability, token_len-2, axis=1, batch_dims=1)[:,0]\n",
    "        # labels_2 = probability[Time - 1, :, length-2]#last true elem\n",
    "        labels_1=tf.gather(probability, token_len-1, axis=1, batch_dims=1)[:,0]\n",
    "        # labels_1 = probability[Time - 1, :, length-1] #blank\n",
    "        labels_prob = labels_2 + labels_1  # sum of the two possible ends : last elem of token or blank\n",
    "        return labels_prob\n",
    "\n",
    "    alpha = collect_label(alphas)\n",
    "    beta = collect_label(betas)\n",
    "    # tf.print(\"alpha final\",  alpha,summarize=-1)\n",
    "\n",
    "    tf.print(\"alpha\",alpha)\n",
    "    # tf.print(\"beta\",beta)\n",
    "\n",
    "    H = -beta / (alpha+eps) + tf.math.log(alpha + eps)\n",
    "    costs = -tf.math.log(alpha + eps)\n",
    "    valFinal = tf.reduce_mean(costs )# - 0.2*H)\n",
    "    # tf.print(\"H\",H,summarize=-1)\n",
    "    tf.print(\"valFinal\",valFinal,summarize=-1)\n",
    "    # tf.print(\"valFinal\",valFinal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_batch_dot(alpha_t, rec):\n",
    "    '''\n",
    "    alpha_t: (batch, 2U+1)\n",
    "    rec: (batch, 2U+1, 2U+1)\n",
    "    '''\n",
    "    # tf.print(\"log_batch_dot\")\n",
    "\n",
    "    eps_nan = -1e8\n",
    "    # a+b\n",
    "    _sum = alpha_t[:, :, None] + rec\n",
    "    _max_sum = tf.reduce_max(_sum, axis=1)# 2U+1\n",
    "    nz_mask1 = tf.greater(_max_sum, eps_nan) # max > eps_nan\n",
    "    nz_mask2 = tf.greater(_sum, eps_nan)     # item > eps_nan\n",
    "    # tf.print(\"nz_mask1\",nz_mask1,nz_mask1.shape,summarize=-1)\n",
    "    # tf.print(\"nz_mask2\",nz_mask2,nz_mask2.shape,summarize=-1)\n",
    "\n",
    "    # a+b-max\n",
    "    _sum = _sum - _max_sum[:, None]\n",
    "\n",
    "    # exp\n",
    "    _exp = tf.zeros_like(_sum,dtype=tf.float32)#.type(floatX)\n",
    "    # _exp[nz_mask2] = tf.exp(_sum[nz_mask2])\n",
    "    _exp = tf.tensor_scatter_nd_update(_exp,tf.where(nz_mask2),tf.exp(_sum[nz_mask2]))\n",
    "    # tf.print(\"_exp\",_exp,tf.shape(_exp),\"exp__\",summarize=-1)\n",
    "\n",
    "    # sum exp\n",
    "    _sum_exp = tf.reduce_sum(_exp, axis=1)\n",
    "\n",
    "    out = tf.ones_like(_max_sum,dtype=tf.float32) * eps_nan\n",
    "    # out[nz_mask1] = tf.math.log(_sum_exp[nz_mask1]) + _max_sum[nz_mask1]\n",
    "    out = tf.tensor_scatter_nd_update(out,tf.where(nz_mask1),tf.math.log(_sum_exp[nz_mask1]) + _max_sum[nz_mask1])\n",
    "\n",
    "    return out\n",
    "\n",
    "def log_sum_exp_axis(a, uniform_mask=None, dim=0):\n",
    "    assert dim == 0\n",
    "    \"\"\"\n",
    "    a : [2,batch]\n",
    "    \"\"\"\n",
    "    eps_nan = -1e8\n",
    "    eps = 1e-26\n",
    "    _max = tf.reduce_max(a, axis=dim)\n",
    "\n",
    "    if not uniform_mask is None:\n",
    "        nz_mask2 = tf.greater(a, eps_nan) * uniform_mask\n",
    "        nz_mask1 = tf.greater(_max, eps_nan) * tf.greater_equal(tf.reduce_max(uniform_mask, axis=dim), 1)\n",
    "    else:\n",
    "        nz_mask2 = tf.greater(a, eps_nan) #\n",
    "        nz_mask1 = tf.greater(_max, eps_nan)\n",
    "    # tf.print(\"nz_mask2.shape\",tf.shape(nz_mask2))\n",
    "    # tf.print(\"nz_mask1.shape\",tf.shape(nz_mask1))\n",
    "    # a-max\n",
    "    a = a - _max[None]\n",
    "\n",
    "    # exp\n",
    "    _exp_a = tf.zeros_like(a,dtype=tf.float32)\n",
    "    # _exp_a[nz_mask2] = tf.exp(a[nz_mask2])\n",
    "    _exp_a = tf.tensor_scatter_nd_update(_exp_a,tf.where(nz_mask2),tf.exp(a[nz_mask2]))\n",
    "\n",
    "    # sum exp\n",
    "    _sum_exp_a = tf.reduce_sum(_exp_a, axis=dim)\n",
    "\n",
    "    out = tf.ones_like(_max,dtype=tf.float32) * eps_nan\n",
    "    # out[nz_mask1] =\n",
    "    summ = _sum_exp_a[nz_mask1]\n",
    "    maxx = _max[nz_mask1]\n",
    "    # tf.print(nz_mask1)\n",
    "    val = tf.math.log( summ+ eps) + maxx\n",
    "    out = tf.tensor_scatter_nd_update(out,tf.where(nz_mask1),val)\n",
    "    return out\n",
    "\n",
    "def log_sum_exp(*arrs):\n",
    "    \"\"\"\n",
    "    probability :\n",
    "    :param arrs:[batch], [batch]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "#    return T.max(a.clone(), b.clone()) + T.log1p(T.exp(-T.abs(a.clone()-b.clone())))\n",
    "    c = tf.concat(list(map(lambda x:x[None], arrs)), axis=0) #[2,batch]\n",
    "    # tf.print(\"c.shape\",tf.shape(c))\n",
    "    return log_sum_exp_axis(c, dim=0)\n",
    "\n",
    "@tf.function\n",
    "def ctc_ent_loss_log(pred, token, pred_len,token_len, blank=0):\n",
    "    '''\n",
    "    :param pred: ( batch,Time, voca_size+1)\n",
    "    :param pred_len: (batch)\n",
    "    :param token: (batch, U), U : length maximal of sequence in batch, padded with ?, length known with token_len\n",
    "    :param token_len: (batch)\n",
    "    # blank seem the index of the blank, but use T.zeros for token_with_blank\n",
    "\n",
    "    :out alpha: (Time, batch, 2U+1) ∑p(π|x)\n",
    "    :out beta: (Time, batch, 2U+1)  ∑p(π|x)logp(π|x)\n",
    "    :out H: -beta/alpha+log(alpha)\n",
    "    '''\n",
    "    pred = tf.transpose(pred,[1,0,2]) # ( Time,batch, voca_size+1)\n",
    "    pred = tf.math.log_softmax(pred)\n",
    "    Time, batch = tf.shape(pred)[0], tf.shape(pred)[1]\n",
    "    U = tf.shape(token)[1]\n",
    "    eps = 1E-8\n",
    "    eps_nan = -1e8\n",
    "\n",
    "    # token_with_blank\n",
    "    # \"view\" is a kind of reshape with sharing memory, but here the original concat is not kept, so same as reshape i guess\n",
    "    token_with_blank = tf.concat((tf.zeros([batch, U, 1],dtype=tf.int32), tf.cast(token[:, :, tf.newaxis],tf.int32)),axis=2)  # (batch, U,2)\n",
    "    token_with_blank = tf.reshape(token_with_blank,[batch,-1])# (batch, 2*U)\n",
    "    # add a blank at the end of elems\n",
    "    token_with_blank = tf.concat((token_with_blank, tf.zeros([batch, 1],dtype=tf.int32)), axis=1)  # (batch, 2U+1)\n",
    "    # token_with_blank: [blank, index_e1,blank, index_e2, .... eU, blank]\n",
    "\n",
    "    length = tf.shape(token_with_blank)[1]  # 2U+1\n",
    "\n",
    "    # take the scores of elements of the token, at each step for each batch\n",
    "    \"\"\"\n",
    "    In[24]:    a\n",
    "    Out[24]:\n",
    "    <shape=(1 (Time), 2 (batch), 5),\n",
    "    array([[[0.9688189 , 0.01720107, 0.5965241 , 0.76499116, 0.6780566 ],\n",
    "            [0.08494377, 0.34031677, 0.92728245, 0.83308065, 0.17265546]]],\n",
    "    In[25]:\n",
    "    tf.gather(a,tf.convert_to_tensor([\n",
    "                                        [0,2,4], #batch1\n",
    "                                        [0,1,4]] #batch2\n",
    "                                    )[tf.newaxis,:],axis=2,batch_dims=2)\n",
    "    Out[25]:\n",
    "    <shape=(1, 2, 3),\n",
    "    array([[[0.9688189 , 0.5965241 , 0.6780566 ],\n",
    "            [0.08494377, 0.34031677, 0.17265546]]]\n",
    "    \"\"\"\n",
    "    pred = tf.gather(pred,tf.repeat(token_with_blank[tf.newaxis,:,:],Time,axis=0),axis=2,batch_dims=2)  # (T, batch, 2U+1)]\n",
    "    # print(pred)\n",
    "\n",
    "    # ## recurrence relation\n",
    "    # take elements with (not) consecutive same token -> 'aa' -> need to pass by the blank\n",
    "    # True/1 = different, False/0 = same\n",
    "    consecutiveDifferent = tf.cast(tf.not_equal(token_with_blank[:, :-2], token_with_blank[:, 2:]),tf.float32)\n",
    "\n",
    "    # pad with two blanks on the left\n",
    "    consecutiveDifferent = tf.concat(( tf.zeros((batch, 2),dtype=tf.float32), consecutiveDifferent), axis=1)\n",
    "\n",
    "    # elements not blank in the GT (one on two)\n",
    "    notBlanksToken = tf.cast(tf.not_equal(token_with_blank, blank),tf.float32)\n",
    "    # kind of mask : True = Not consecutive identic elements, False = consecutive identic elements: can skip the blank,\n",
    "    sec_diag = consecutiveDifferent * notBlanksToken  # (batch, 2U+1)\n",
    "\n",
    "    # m_eye : identity matrix for k=0, ones of are shifted by k\n",
    "\n",
    "    # recurrence_relation : (batch, 2U+1, 2U+1)\n",
    "    recurrence_relation = \\\n",
    "        tf.repeat((tf.eye(length) + m_eye(length, k=1))[tf.newaxis], repeats = batch,axis=0) + \\\n",
    "        tf.repeat(  m_eye(length, k=2)[tf.newaxis],                  repeats = batch,axis=0) * sec_diag[:, tf.newaxis, :]\n",
    "    # tf.print(\"recurrence_relation\\n\",recurrence_relation,summarize=-1)\n",
    "    recurrence_relation = eps_nan * (tf.ones_like(recurrence_relation) - recurrence_relation)\n",
    "\n",
    "    ### end recurrence relation\n",
    "\n",
    "    # alpha\n",
    "    # initialisation, two first char of seq (blank and first elem) initalised with the direct score of prediction\n",
    "    \"\"\"\n",
    "    T.cat((pred[0,:,:2],T.zeros(batch,5-1)),dim=1)\n",
    "        tensor([\n",
    "        [0.9503, 0.3439, 0.0000, 0.0000, 0.0000, 0.0000], # 1er elem of batch\n",
    "        [0.7988, 0.6641, 0.0000, 0.0000, 0.0000, 0.0000]]) # 2nd elem of batch\n",
    "\n",
    "    --> For t = 0, take blank and first elem of token\n",
    "    \"\"\"\n",
    "    alpha_t = tf.concat((pred[0, :, :2], tf.ones([batch, 2 * U + 1 - 2],dtype=tf.float32)*eps_nan), axis=1)  # (batch, 2U+1)\n",
    "    # same but score*log(score) : this is the regularisation\n",
    "    beta_t = tf.concat((pred[0, :, :2] * tf.math.log(-pred[0, :, :2]+eps), tf.ones([batch, 2 * U - 1],dtype=tf.float32)*eps_nan),\n",
    "                   axis=1)  # (batch, 2U+1)\n",
    "    # tf.print(\"alpha t 0\",alpha_t,tf.shape(alpha_t))\n",
    "\n",
    "    alphas = tf.zeros([Time,batch,length])\n",
    "    betas = tf.zeros([Time,batch,length])\n",
    "    # will store all the alphas for all times, first do it for t=0\n",
    "    \"\"\"\n",
    "    alpha_t =  tf.convert_to_tensor([[1,2,0,0],[5,6,0,0]],dtype=tf.float32)\n",
    "    tf.tensor_scatter_nd_update(tf.zeros(shape=(3, 2, 4)), [[0]],alpha_t[tf.newaxis])\n",
    "    Out:\n",
    "    <tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n",
    "    array([[[1., 2., 0., 0.],\n",
    "            [5., 6., 0., 0.]],\n",
    "           [[0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0.]],\n",
    "           [[0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0.]]], dtype=float32)>\n",
    "    \"\"\"\n",
    "    alphas = tf.tensor_scatter_nd_update(alphas, [[0]], alpha_t[tf.newaxis])\n",
    "    # betas = tf.tensor_scatter_nd_update(betas, [[0]], beta_t[tf.newaxis])\n",
    "    # tf.print(\"pred\",pred,summarize=-1)\n",
    "    # print(tf.executing_eagerly())\n",
    "\n",
    "    def do(t, alpha_t, beta_t, alphas, betas):\n",
    "        alpha_t = log_batch_dot(alpha_t, recurrence_relation) + pred[t, :, :]\n",
    "        beta_t = log_sum_exp(log_batch_dot(beta_t, recurrence_relation) + pred[t, :, :],\n",
    "                             tf.math.log(-pred[t] + eps) + alpha_t)\n",
    "        alphas = tf.tensor_scatter_nd_update(alphas, [[t]], alpha_t[tf.newaxis])\n",
    "        betas = tf.tensor_scatter_nd_update(betas, [[t]], beta_t[tf.newaxis])\n",
    "        return t + 1, alpha_t, beta_t, alphas, betas\n",
    "\n",
    "    i = tf.constant(0)\n",
    "    i, alpha_t, beta_t, alphas, betas, = tf.while_loop(lambda i, at, bt, a, b: i < Time, do,\n",
    "                                                       [i, alpha_t, beta_t, alphas, betas, ])\n",
    "\n",
    "    # dynamic programming\n",
    "    # # (T, batch, 2U+1)\n",
    "    # for t in tf.range(1, Time,dtype=tf.int32):\n",
    "    #     alpha_t = log_batch_dot(alpha_t, recurrence_relation)+ pred[t,:,:]\n",
    "    #     beta_t = log_sum_exp(log_batch_dot(beta_t, recurrence_relation) + pred[t,:,:],\n",
    "    #                          tf.math.log(-pred[t]+eps) + alpha_t)\n",
    "    #\n",
    "    #     alphas = tf.tensor_scatter_nd_update(alphas, [[t]], alpha_t[tf.newaxis])\n",
    "    #     betas = tf.tensor_scatter_nd_update(betas, [[t]], beta_t[tf.newaxis])\n",
    "    #     # tf.print(\"alphas t\", t,alphas, tf.shape(alphas),summarize=-1)\n",
    "    #\n",
    "    # tf.print(\"alphas\", alphas,tf.shape(alphas))\n",
    "\n",
    "    # tf.print(\"alphas\", Time, alphas)\n",
    "    # tf.print(\"beta_t\", Time-1, beta_t)\n",
    "\n",
    "    def collect_label(probability):\n",
    "        \"\"\"\n",
    "\n",
    "        :param probability: [time,batch, 2U+1]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        probability = tf.transpose(probability, [1, 0, 2])\n",
    "        # pred len : [batch,1]\n",
    "        # print(probability)\n",
    "        # print(pred_len)\n",
    "        probability = tf.gather(probability, pred_len - 1, axis=1, batch_dims=1)[:, 0, :]\n",
    "        # probability = probability[Time - 1, :, :]  # last true elem\n",
    "        # token_len : [batch,1]\n",
    "        realTokenlen = (token_len * 2 + 1)\n",
    "        labels_2 = tf.gather(probability,  realTokenlen- 2, axis=1, batch_dims=1)[:, 0]\n",
    "        # labels_2 = probability[:, length-2]#last true elem\n",
    "        labels_1 = tf.gather(probability, realTokenlen - 1, axis=1, batch_dims=1)[:, 0]\n",
    "        # labels_1 = probability[:, length-1] #blank\n",
    "\n",
    "        labels_2 = tf.repeat(labels_2[tf.newaxis,:],batch,axis=0)#last true elem\n",
    "        labels_1 = tf.repeat(labels_1[tf.newaxis,:],batch,axis=0)#last true elem\n",
    "        # labels_1 = tf.repeat(probability[token_len - 1,tf.newaxis, :, length-1],batch,axis=0)#last true elem\n",
    "        # tf.print(\"labels_2\", tf.shape(labels_2))\n",
    "        # tf.print(\"labels_1\", tf.shape(labels_1))\n",
    "\n",
    "        labels_prob = log_sum_exp(labels_2,labels_1)  # sum of the two possible ends : last elem of token or blank\n",
    "        return labels_prob\n",
    "\n",
    "    alpha = collect_label(alphas)\n",
    "    # tf.print(\"alpha\",alpha,tf.shape(alpha))\n",
    "    beta = collect_label(betas)\n",
    "    # tf.print(\"alpha final\",  alpha,summarize=-1)\n",
    "\n",
    "    # tf.print(\"alpha\",alpha)\n",
    "    # tf.print(\"beta\",beta)\n",
    "\n",
    "    H = tf.exp(beta-alpha)+alpha\n",
    "    costs = -alpha\n",
    "    valFinal = 0.8*tf.reduce_sum(costs) - 0.2*tf.reduce_sum(H)\n",
    "    # tf.print(\"H\",H,summarize=-1)\n",
    "    # tf.print(\"valFinal\",valFinal,summarize=-1)\n",
    "    # tf.print(\"valFinal\",valFinal)\n",
    "    return valFinal\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}